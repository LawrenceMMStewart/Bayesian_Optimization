#Set Random Seed:
srand(1234)


#uniform.jl

"""
Create a NxM uniformly distributed matrix using the method rand()*(b-a)-a where a,b are the Unif Parameters.

Arguments
---------
a
    Lower Bound 
b
    Upper Bound
N
    Row Dimension
M 
    Column Dimension

"""
function uniform(a,b,N,M)
    #Returns a NxM uniformly distributed matrix
    #for random number in range [a,b] we note that it can be generated by: rand() * (b-a) - a 
    #where rand() prints real numbers:
    rand(N,M)*(b-a)+a
end



#sigmoid.jl

"""
Return the sigmoid of x:

Arguments
---------
x
    input
h
    hyperparemeter

"""


function sigmoid(x,h)
    return 1/(1+exp(-h*x))
end


#sigmoid_deriv.jl

"""
Return the sigmoid of x:

Arguments
---------
x
    input
h
    hyperparemeter

"""


function sigmoid_deriv(x,h)
    return (1/(1+exp(-h*x)))*(1-1/(1+exp(-h*x)))
end









#XOR_Sequence.jl

"""
Return the temporal XOR_Sequence of length n defined by two random values from {0,1}
then followed by the XOR function of the previous two elements:

Arguments
---------
n
    number of terms in the XOR_Sequence

"""

function XOR_Sequence(n)
    if n % 3 !=0
        println("Input n is not divisible by 3 so output will have an incomplete cycle")
    end
    
    seq=zeros(n)

    for i=1:n
        if i %3 !=0
            seq[i]=rand([0,1])
        else
            
            if seq[i-1]==seq[i-2]
                seq[i]=0
            else
                seq[i]=1
            end
        
        end
    end
    
    return seq
end






# Here we will create the reccurent neural network:

Layer_1=uniform(0,1,1,2) #Could be the other way round also could set them to 0.5 instead
Layer_2=uniform(0,1,2,1)
recurrent_layer=uniform(0,1,2,2)



Seq_Len=100


function hyper_curry(h)
    return (x->sigmoid(x,h))
end

function hyper_curry_deriv(h)
    return (x->sigmoid_deriv(x,h))
end

node_function=hyper_curry(1)
node_deriv=hyper_curry_deriv(1)


epochs=100
learning_rate=0.01  

function Train_Reccurent_Net_Loop(epochs,Layer_1,Layer_2,recurrent_layer,learning_rate,node_function,node_deriv,Seq_Len)
    
    seq=XOR_Sequence(Seq_Len)
    context_units_out=transpose([0.5, 0.5]) #Pre-intialise the first output for the context units:
        
    
    SE=0

    Starting_Square_Error=zeros(Seq_Len-3)
    Final_Square_Error=zeros(Seq_Len-3)
    
    for k=1:epochs
        for i=1:length(seq)-3

            a_2=map(node_function,[seq[i]]*Layer_1+context_units_out*recurrent_layer)
            a_3=map(node_function,a_2*Layer_2)
            direct_error=a_3-seq[i+1]
            SE=0.5*sum(direct_error.*direct_error)
            context_units_out=map(node_function,a_2*ones(2,2))

            #This is the foward pass for an individual point
            # if i%3==0
            #     println(string("Square Error = ",SE,"\r"))
            # end

            #Begin backpropagation


            """
            Add a counter that adds up all the errors on the XOR/3rd terms
            for each epoch by setting a threshold for 0, 1 and the seeing
            if errors decrease as we move onwards.

            Also add a counter for overall errors and see about that just for 
            curiosity (but delete after)

            """

            delta_outer=-1.0*direct_error.*map(node_deriv,a_2*Layer_2)
            delta_inner=delta_outer*transpose(Layer_2).*map(node_deriv,[seq[i]]*Layer_1)


            Layer_2 +=learning_rate*transpose(a_2)*delta_outer
            Layer_1 +=learning_rate*transpose([seq[i]])*delta_inner

            
            recurrent_layer +=transpose(context_units_out)*delta_outer*transpose(Layer_2).*map(node_deriv,context_units_out*recurrent_layer)


            if k==1
                Starting_Square_Error[i]=SE
            end


            if k==epochs
                Final_Square_Error[i]=SE
            end
        


        end
        println(string("Square Error = ",SE,"\r"))
    end
    return [Starting_Square_Error, Final_Square_Error]
end






P=Train_Reccurent_Net_Loop(epochs,Layer_1,Layer_2,recurrent_layer,learning_rate,node_function,node_deriv,Seq_Len)
y0=P[1]
yend=P[2]



xvals=[i for i=1:length(y0)]



using PyPlot

plot(xvals,y0,label="SE of First Epoch",alpha=0.4)
plot(xvals,yend,label="SE of Last Epoch",alpha=0.9)
title("SE Plot For Each Term in XOR Sequence ")
xlabel(L"$n$")
ylabel(L"${XOR}_n$")
legend(loc="upper right",fancybox="true")
axis("tight")
grid("off")
show()




