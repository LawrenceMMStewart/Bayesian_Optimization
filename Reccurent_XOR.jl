#Set Random Seed:
srand(1234)


#uniform.jl

"""
Create a NxM uniformly distributed matrix using the method rand()*(b-a)-a where a,b are the Unif Parameters.

Arguments
---------
a
    Lower Bound 
b
    Upper Bound
N
    Row Dimension
M 
    Column Dimension

"""
function uniform(a,b,N,M)
    #Returns a NxM uniformly distributed matrix
    #for random number in range [a,b] we note that it can be generated by: rand() * (b-a) - a 
    #where rand() prints real numbers:
    rand(N,M)*(b-a)+a
end



#sigmoid.jl

"""
Return the sigmoid of x:

Arguments
---------
x
    input
h
    hyperparemeter

"""


function sigmoid(x,h)
    return 1/(1+exp(-h*x))
end


#sigmoid_deriv.jl

"""
Return the sigmoid of x:

Arguments
---------
x
    input
h
    hyperparemeter

"""


function sigmoid_deriv(x,h)
    return (1/(1+exp(-h*x)))*(1-1/(1+exp(-h*x)))
end









#XOR_Sequence.jl

"""
Return the temporal XOR_Sequence of length n defined by two random values from {0,1}
then followed by the XOR function of the previous two elements:

Arguments
---------
n
    number of terms in the XOR_Sequence

"""

function XOR_Sequence(n)
    if n % 3 !=0
        println("Input n is not divisible by 3 so output will have an incomplete cycle")
    end
    
    seq=zeros(n)

    for i=1:n
        if i %3 !=0
            seq[i]=rand([0,1])
        else
            
            if seq[i-1]==seq[i-2]
                seq[i]=0
            else
                seq[i]=1
            end
        
        end
    end
    
    return seq
end






# Here we will create the reccurent neural network:

Layer_1=uniform(0,1,1,2) 
Layer_2=uniform(0,1,2,1)
recurrent_layer=uniform(0,1,2,2)



Seq_Len=100



function Train_Reccurent_Net_Loop(epochs,Layer_1,Layer_2,recurrent_layer,learning_rate,node_function,node_deriv,Seq_Len)
    
    seq=XOR_Sequence(Seq_Len)
    context_units_out=transpose([0.5, 0.5]) #Pre-intialise the first output for the context units:
        
    
    SE=0

    Starting_Square_Error=zeros(Seq_Len-3)
    Final_Square_Error=zeros(Seq_Len-3)
    
    for k=1:epochs
        for i=1:length(seq)-3

            a_2=map(node_function,[seq[i]]*Layer_1+context_units_out*recurrent_layer)
            a_3=map(node_function,a_2*Layer_2)
            direct_error=a_3-seq[i+1]
            SE=0.5*sum(direct_error.*direct_error)
            context_units_out=map(node_function,a_2*ones(2,2))

            #This is the foward pass for an individual point
            # if i%3==0
            #     println(string("Square Error = ",SE,"\r"))
            # end

            #Begin backpropagation

            delta_outer=-1.0*direct_error.*map(node_deriv,a_2*Layer_2)
            delta_inner=delta_outer*transpose(Layer_2).*map(node_deriv,[seq[i]]*Layer_1)


            Layer_2 +=learning_rate*transpose(a_2)*delta_outer
            Layer_1 +=learning_rate*transpose([seq[i]])*delta_inner

            
            recurrent_layer +=transpose(context_units_out)*delta_outer*transpose(Layer_2).*map(node_deriv,context_units_out*recurrent_layer)


            if k==1
                Starting_Square_Error[i]=SE
            end


            if k==epochs
                Final_Square_Error[i]=SE
            end
        


        end
        println(string("Square Error = ",SE,"\r"))
    end
    return [Starting_Square_Error, Final_Square_Error]
end








#Test 1 ======================================================================================================

#Here we run a temporal XOR example to show how the reccurent net learns with time:

##Run


# function hyper_curry(h)
#     return (x->sigmoid(x,h))
# end

# function hyper_curry_deriv(h)
#     return (x->sigmoid_deriv(x,h))
# end

# node_function=hyper_curry(1)
# node_deriv=hyper_curry_deriv(1)

# epochs=100
# learning_rate=0.01  



# P=Train_Reccurent_Net_Loop(epochs,Layer_1,Layer_2,recurrent_layer,learning_rate,node_function,node_deriv,Seq_Len)
# y0=P[1]
# yend=P[2]
# xvals=[i for i=1:length(y0)]



# using PyPlot

# plot(xvals,y0,label="SE of First Epoch",alpha=0.4)
# plot(xvals,yend,label="SE of Last Epoch",alpha=0.9)
# title("SE Plot For Each Term in XOR Sequence ")
# xlabel(L"$n$")
# ylabel(L"${XOR}_n$")
# legend(loc="upper right",fancybox="true")
# axis("tight")
# grid("off")
# show()

##End run

# ========================================================================================================


















































#For some reason this code shows that thresholding does nothing, same error each time, which 
#is the seed area. Think about for a while then if no idea delete this 


# function Train_Reccurent_Net_Loop_Threshold(epochs,Layer_1,Layer_2,recurrent_layer,learning_rate,node_function,node_deriv,Seq_Len)
    

#     #We assume trivially by definition the threshold value of 0.5
#     seq=XOR_Sequence(Seq_Len)
#     context_units_out=transpose([0.5, 0.5]) #Pre-intialise the first output for the context units:
        
    
#     SE=0
#     thres=0

#     Errors=zeros(epochs)
#     epoch_error=0
    
#     for k=1:epochs
#         epoch_error=0
#         for i=1:length(seq)-3
            
#             a_2=map(node_function,[seq[i]]*Layer_1+context_units_out*recurrent_layer)
#             a_3=map(node_function,a_2*Layer_2)
#             direct_error=a_3-seq[i+1]

#             #See whether the direct_error is within 0.5 (as we round up or down wrt threshold)
#             if direct_error[1]>=0.5
#                 epoch_error+=1
#                 println(string("Error on position ",i))
#             end


            
#             SE=0.5*sum(direct_error.*direct_error)
#             context_units_out=map(node_function,a_2*ones(2,2))


#             #Begin backpropagation



#             delta_outer=-1.0*direct_error.*map(node_deriv,a_2*Layer_2)
#             delta_inner=delta_outer*transpose(Layer_2).*map(node_deriv,[seq[i]]*Layer_1)


#             Layer_2 +=learning_rate*transpose(a_2)*delta_outer
#             Layer_1 +=learning_rate*transpose([seq[i]])*delta_inner

            
#             recurrent_layer +=transpose(context_units_out)*delta_outer*transpose(Layer_2).*map(node_deriv,context_units_out*recurrent_layer)


        


#         end
#         println(string("Square Error = ",SE,"\r"))
#         Errors[k]=epoch_error
#     end
#     return Errors
# end



# print(Train_Reccurent_Net_Loop_Threshold(epochs,Layer_1,Layer_2,recurrent_layer,learning_rate,node_function,node_deriv,Seq_Len))



#Here we will add a threshold value to see  how the errors change as the network develops:

#We will take the threshold value to be 0.5:


